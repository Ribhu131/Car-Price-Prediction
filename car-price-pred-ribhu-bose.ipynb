{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Project Title : Car Price Prediction**","metadata":{}},{"cell_type":"markdown","source":"**About the data:** \n\nWith the rise in the variety of cars with differentiated capabilities and features such as model, production year, category, brand, fuel type, engine volume, mileage, cylinders, colour, airbags and many more, we are bringing a car price prediction challenge for all. We all aspire to own a car within budget with the best features available. To solve the price problem we have created a dataset of 19237 for the training dataset and 8245 for the test dataset.","metadata":{}},{"cell_type":"markdown","source":"**Attribute Information:**\n* ID\n* Price: price of the care(Target Column)\n* Levy\n* Manufacturer\n* Model\n* Prod. year\n* Category\n* Leather interior\n* Fuel type\n* Engine volume\n* Mileage\n* Cylinders\n* Gear box type\n* Drive wheels\n* Doors\n* Wheel\n* Color\n* Airbags","metadata":{}},{"cell_type":"markdown","source":"# **Data Cleaning and Pre-processing**","metadata":{}},{"cell_type":"markdown","source":"**Importing the all the libraies and loading the dataset**","metadata":{}},{"cell_type":"code","source":"# Importing the libraries\nimport numpy as np               # linear algebra\nimport pandas as pd              # data processing\nimport seaborn as sns            # Graph ploting \nimport matplotlib.pyplot as plt  # Graph ploting \n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:22:13.846214Z","iopub.execute_input":"2022-07-12T20:22:13.846898Z","iopub.status.idle":"2022-07-12T20:22:15.907890Z","shell.execute_reply.started":"2022-07-12T20:22:13.846775Z","shell.execute_reply":"2022-07-12T20:22:15.906416Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing the dataset\ndf = pd.read_csv('/kaggle/input/car-price-prediction-challenge/car_price_prediction.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:22:45.764228Z","iopub.execute_input":"2022-07-12T20:22:45.764750Z","iopub.status.idle":"2022-07-12T20:22:45.905450Z","shell.execute_reply.started":"2022-07-12T20:22:45.764710Z","shell.execute_reply":"2022-07-12T20:22:45.904073Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Seaborn setting \ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", rc=custom_params)\n# To ignore irrelevent warnigns \nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:23:07.854890Z","iopub.execute_input":"2022-07-12T20:23:07.855425Z","iopub.status.idle":"2022-07-12T20:23:07.864675Z","shell.execute_reply.started":"2022-07-12T20:23:07.855381Z","shell.execute_reply":"2022-07-12T20:23:07.863009Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Lets take a glimpse of the overall data and clean the data**","metadata":{}},{"cell_type":"code","source":"# Lets preview the data \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:23:16.914531Z","iopub.execute_input":"2022-07-12T20:23:16.914954Z","iopub.status.idle":"2022-07-12T20:23:16.949975Z","shell.execute_reply.started":"2022-07-12T20:23:16.914919Z","shell.execute_reply":"2022-07-12T20:23:16.948490Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Get the basic info of the data\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:23:27.231746Z","iopub.execute_input":"2022-07-12T20:23:27.232394Z","iopub.status.idle":"2022-07-12T20:23:27.297603Z","shell.execute_reply.started":"2022-07-12T20:23:27.232336Z","shell.execute_reply":"2022-07-12T20:23:27.296225Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(f'The shape of the data set is {df.shape}.')","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:23:39.263799Z","iopub.execute_input":"2022-07-12T20:23:39.264247Z","iopub.status.idle":"2022-07-12T20:23:39.271960Z","shell.execute_reply.started":"2022-07-12T20:23:39.264211Z","shell.execute_reply":"2022-07-12T20:23:39.270784Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Lets check for the number of unique value in each column\ndf.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:23:47.642787Z","iopub.execute_input":"2022-07-12T20:23:47.643225Z","iopub.status.idle":"2022-07-12T20:23:47.684313Z","shell.execute_reply.started":"2022-07-12T20:23:47.643191Z","shell.execute_reply":"2022-07-12T20:23:47.683080Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Point:** The number of rows in dataset are 19237 but number of unique IDs are less 18924 therefore it shows that there might be duplicate rows.","metadata":{}},{"cell_type":"code","source":"# Check for the duplicate rows\nprint(f\"The total duplicate rows are {df.duplicated().sum()}.\")\n\n# Drop all duplicates in the DataFrame\ndf = df.drop_duplicates()\n      \n# Check for the duplicate rows\nprint(f\"The total duplicate rows after deleting duplicates are {df.duplicated().sum()}.\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:24:01.464565Z","iopub.execute_input":"2022-07-12T20:24:01.465330Z","iopub.status.idle":"2022-07-12T20:24:01.576996Z","shell.execute_reply.started":"2022-07-12T20:24:01.465264Z","shell.execute_reply":"2022-07-12T20:24:01.575210Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Lets find the number of NaN values in each column\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:24:11.373504Z","iopub.execute_input":"2022-07-12T20:24:11.373977Z","iopub.status.idle":"2022-07-12T20:24:11.412888Z","shell.execute_reply.started":"2022-07-12T20:24:11.373943Z","shell.execute_reply":"2022-07-12T20:24:11.411677Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Lets see the unique values and frequency of Levy feature\ndf['Levy'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:24:22.772768Z","iopub.execute_input":"2022-07-12T20:24:22.773221Z","iopub.status.idle":"2022-07-12T20:24:22.787403Z","shell.execute_reply.started":"2022-07-12T20:24:22.773188Z","shell.execute_reply":"2022-07-12T20:24:22.786396Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Remove the dash with 0 in Levy and change datatype to integer\ndf.loc[df['Levy'] == '-', 'Levy'] = 0      # Replace - with 0 \ndf['Levy'] = df['Levy'].astype(int)        # Change datatype from string to intger\ndf.info()                                  # Lets check the info again ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:24:36.874515Z","iopub.execute_input":"2022-07-12T20:24:36.875019Z","iopub.status.idle":"2022-07-12T20:24:36.928231Z","shell.execute_reply.started":"2022-07-12T20:24:36.874978Z","shell.execute_reply":"2022-07-12T20:24:36.927281Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Lets see the unique values and frequency of Mileage feature\ndf['Mileage'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:24:48.652968Z","iopub.execute_input":"2022-07-12T20:24:48.653437Z","iopub.status.idle":"2022-07-12T20:24:48.670552Z","shell.execute_reply.started":"2022-07-12T20:24:48.653398Z","shell.execute_reply":"2022-07-12T20:24:48.669588Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Remove km from  Mileage columne and convert the data in integer\ndf['Mileage'] = df['Mileage'].apply(lambda x : int(x.split()[0]))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:24:58.283689Z","iopub.execute_input":"2022-07-12T20:24:58.284662Z","iopub.status.idle":"2022-07-12T20:24:58.317985Z","shell.execute_reply.started":"2022-07-12T20:24:58.284602Z","shell.execute_reply":"2022-07-12T20:24:58.316732Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Lets see the unique values and frequency of Mileage feature\ndf['Engine volume'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:25:13.744782Z","iopub.execute_input":"2022-07-12T20:25:13.745208Z","iopub.status.idle":"2022-07-12T20:25:13.762095Z","shell.execute_reply.started":"2022-07-12T20:25:13.745172Z","shell.execute_reply":"2022-07-12T20:25:13.760367Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"turbo_eng = df[df['Engine volume'].str.contains('Turbo')]['Engine volume'].value_counts().sum()\nprint(f\"Out of {df.shape[0]} cars {turbo_eng} - {round((100*turbo_eng)/df.shape[0])}% have turbo engine.\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:25:24.641321Z","iopub.execute_input":"2022-07-12T20:25:24.641736Z","iopub.status.idle":"2022-07-12T20:25:24.670863Z","shell.execute_reply.started":"2022-07-12T20:25:24.641703Z","shell.execute_reply":"2022-07-12T20:25:24.669557Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Since 10% contains turbo engines ignore it for now is not good idea, lets create a new feature that tells whether the car has turbo engine or not.**","metadata":{}},{"cell_type":"code","source":"# Let create a new feature to show whether the engine is turbo or not\ndf['engine_turbo'] = np.where(df['Engine volume'].str.contains('Turbo'), 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:25:38.413812Z","iopub.execute_input":"2022-07-12T20:25:38.414927Z","iopub.status.idle":"2022-07-12T20:25:38.436403Z","shell.execute_reply.started":"2022-07-12T20:25:38.414883Z","shell.execute_reply":"2022-07-12T20:25:38.435276Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Remove Turbo from  Engine volume columne and convert the data in float\ndf['Engine volume'] = df['Engine volume'].apply(lambda x : float(x.split()[0]))\n# Lets check the info again \ndf.info()      ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:25:46.983121Z","iopub.execute_input":"2022-07-12T20:25:46.983592Z","iopub.status.idle":"2022-07-12T20:25:47.038600Z","shell.execute_reply.started":"2022-07-12T20:25:46.983553Z","shell.execute_reply":"2022-07-12T20:25:47.037287Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Lets see the unique values and frequency of Doors feature\ndf['Doors'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:25:56.433400Z","iopub.execute_input":"2022-07-12T20:25:56.433898Z","iopub.status.idle":"2022-07-12T20:25:56.446906Z","shell.execute_reply.started":"2022-07-12T20:25:56.433859Z","shell.execute_reply":"2022-07-12T20:25:56.445538Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# The values in door columns converted in datetime format, lets convert it back to proper format\ndef door(value):\n    if value == '04-May':\n        return \"4-5\"\n    elif value == '02-Mar':\n        return \"2-3\"\n    elif value == '>5':\n        return \"5+\"\n\ndf['Doors'] = df['Doors'].apply(lambda x : door(x))\n# Lets view the door columns again\ndf['Doors'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:26:05.814635Z","iopub.execute_input":"2022-07-12T20:26:05.815066Z","iopub.status.idle":"2022-07-12T20:26:05.840191Z","shell.execute_reply.started":"2022-07-12T20:26:05.815033Z","shell.execute_reply":"2022-07-12T20:26:05.838994Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Change the name of columns in short names\ndf = df.rename(columns={'Price': 'price', 'Levy': 'levy', 'Manufacturer': 'manufacturer', \n                        'Model': 'model', 'Prod. year': 'prd_yr', 'Category': 'category', \n                        'Leather interior': 'leather_intr', 'Fuel type': 'fuel_typ', \n                        'Engine volume': 'engine_vol', 'Mileage': 'mileage(km)' , 'Cylinders': 'cylinders',\n                        'Gear box type': 'gear_box', 'Drive wheels': 'drive_wheels', 'Doors': 'doors',\n                        'Wheel': 'wheel', 'Color' : 'color', 'Airbags' : 'airbags'\n                       })","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:26:17.683806Z","iopub.execute_input":"2022-07-12T20:26:17.684240Z","iopub.status.idle":"2022-07-12T20:26:17.695866Z","shell.execute_reply.started":"2022-07-12T20:26:17.684206Z","shell.execute_reply":"2022-07-12T20:26:17.694898Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Drop the reduntant column ID\ndf = df.drop(['ID'], axis = 1) \n# lets preview the data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:26:25.843404Z","iopub.execute_input":"2022-07-12T20:26:25.843832Z","iopub.status.idle":"2022-07-12T20:26:25.872045Z","shell.execute_reply.started":"2022-07-12T20:26:25.843799Z","shell.execute_reply":"2022-07-12T20:26:25.870537Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Lets define the continous and categorical columns\ncont_col = ['levy', 'prd_yr', 'engine_vol', 'mileage(km)', 'airbags']\ncat_col  = ['manufacturer', 'model', 'category', 'leather_intr', 'fuel_typ', 'gear_box', 'drive_wheels', 'doors', 'color', 'engine_turbo']","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:26:34.564695Z","iopub.execute_input":"2022-07-12T20:26:34.565147Z","iopub.status.idle":"2022-07-12T20:26:34.572081Z","shell.execute_reply.started":"2022-07-12T20:26:34.565114Z","shell.execute_reply":"2022-07-12T20:26:34.570767Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Lets see the unique values and the frequency of values in each categorical columns\nfor col in cat_col:\n    print(f'\\033[1mFrequency for the column: {col}  \\033[0m')\n    print(df[col].value_counts())\n    print('='*50+\"||\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:26:42.221350Z","iopub.execute_input":"2022-07-12T20:26:42.221788Z","iopub.status.idle":"2022-07-12T20:26:42.261682Z","shell.execute_reply.started":"2022-07-12T20:26:42.221754Z","shell.execute_reply":"2022-07-12T20:26:42.260642Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Note**: The column manufacturer and model both are categorical columns but they contains too many categories. For a ML algoritham that much categories will not give good result. So, either we have to drop them or have to transform it into more simpler new features.","metadata":{}},{"cell_type":"markdown","source":"# **Exploratory Data Analysis**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:26:55.623627Z","iopub.execute_input":"2022-07-12T20:26:55.624121Z","iopub.status.idle":"2022-07-12T20:26:55.647034Z","shell.execute_reply.started":"2022-07-12T20:26:55.624082Z","shell.execute_reply":"2022-07-12T20:26:55.646028Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **Univariate Variable Analysis**","metadata":{}},{"cell_type":"code","source":"# Lets define the continous columns\ncont_col = ['price', 'levy',  'engine_vol', 'mileage(km)', 'airbags']\n\n#Plotting Graphs Before treating outliers of continous features\nfor col in cont_col:\n    fig, ax =plt.subplots(1,2, constrained_layout=True)\n    fig.set_size_inches(20, 3)\n    sns.distplot(df[col], ax=ax[0]).set(title=\"Distplot\")\n    sns.boxplot(df[col], ax=ax[1]).set(title=\"Boxplot\")\n    plt.suptitle(f'{col.title()} (Before handling outliers)',weight='bold')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:27:04.834762Z","iopub.execute_input":"2022-07-12T20:27:04.835197Z","iopub.status.idle":"2022-07-12T20:27:08.062464Z","shell.execute_reply.started":"2022-07-12T20:27:04.835164Z","shell.execute_reply":"2022-07-12T20:27:08.061173Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Function to get amount of outliers in numerical columns\ndef outlier_prcnt(df, col_list):\n    for col_name in col_list:\n        q1 = df[col_name].quantile(0.25)\n        q3 = df[col_name].quantile(0.75)\n        iqr = q3-q1  #Interquartile range\n        fence_low  = q1-1.5*iqr\n        fence_high = q3+1.5*iqr\n\n        # Print total outlers and percentage using IQR method\n        outliers = ((df[col_name] > fence_high) | (df[col_name] < fence_low)).sum()\n        total = df[col_name].shape[0]\n        print(f\"Total outliers in {col_name} are: {outliers} - {round(100*(outliers)/total,2)}%.\")\n\n# Lets get the amount of outliers in each numerical columns\noutlier_prcnt(df, cont_col)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:27:17.373793Z","iopub.execute_input":"2022-07-12T20:27:17.374220Z","iopub.status.idle":"2022-07-12T20:27:17.403021Z","shell.execute_reply.started":"2022-07-12T20:27:17.374185Z","shell.execute_reply":"2022-07-12T20:27:17.402044Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# # Removing outliers for price column\n# q1 = df['price'].quantile(0.25)\n# q3 = df['price'].quantile(0.75)\n# IQR = q3 - q1\n# df = df[~((df['price'] < (q1 - 1.5 * IQR)) |(df['price'] > (q3 + 1.5 * IQR)))]","metadata":{"execution":{"iopub.status.busy":"2022-07-12T23:23:43.295414Z","iopub.execute_input":"2022-07-12T23:23:43.297093Z","iopub.status.idle":"2022-07-12T23:23:43.304599Z","shell.execute_reply.started":"2022-07-12T23:23:43.297013Z","shell.execute_reply":"2022-07-12T23:23:43.302766Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# Function to capping the outliers in numerical columns using IQR method\ndef outlier_handle(df, col_list):\n    df_new = df.copy()\n    for col_name in col_list:\n        q1 = df[col_name].quantile(0.25)\n        q3 = df[col_name].quantile(0.75)\n        iqr = q3-q1  #Interquartile range\n        fence_low  = q1-1.5*iqr\n        fence_high = q3+1.5*iqr\n\n        # Capping Outliers using IQR method\n        df_new.loc[:,  col_name] = np.where(df[col_name]> fence_high, fence_high,\n                                         np.where(df[col_name]< fence_low, fence_low,\n                                                  df[col_name]))\n    return df_new\n# Lets get the amount of outliers in each numerical columns\ndf = outlier_handle(df, cont_col)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:27:31.214037Z","iopub.execute_input":"2022-07-12T20:27:31.214555Z","iopub.status.idle":"2022-07-12T20:27:31.242253Z","shell.execute_reply.started":"2022-07-12T20:27:31.214519Z","shell.execute_reply":"2022-07-12T20:27:31.240669Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Plotting Graphs After treating outliers of continous features\nfor col in cont_col:\n    fig, ax =plt.subplots(1,2, constrained_layout=True)\n    fig.set_size_inches(20, 3)\n    sns.distplot(df[col], ax=ax[0]).set(title=\"Distplot\")\n    sns.boxplot(df[col], ax=ax[1]).set(title=\"Boxplot\")\n    plt.suptitle(f'{col.title()} (After handling outliers)',weight='bold')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:27:43.223664Z","iopub.execute_input":"2022-07-12T20:27:43.224096Z","iopub.status.idle":"2022-07-12T20:27:46.231980Z","shell.execute_reply.started":"2022-07-12T20:27:43.224061Z","shell.execute_reply":"2022-07-12T20:27:46.230684Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** Note: From the above density plot of varierty of cars we make the following conclusions.\n\nMileage, levy, and the price are right skewed\nThe varierty of car for particular values of airbags are very high.\nThe varierty of car for particular values of engine volume are very high.","metadata":{}},{"cell_type":"code","source":"# Defining a function to Notate the percent count of each value on the bars\ndef annot_percent(axes):\n    '''Takes axes as input and labels the percent count of each bar in a countplot'''\n    for p in plot.patches:\n        total = sum(p.get_height() for p in plot.patches)/100\n        percent = round((p.get_height()/total),2)\n        x = p.get_x() + p.get_width()/2\n        y = p.get_height()*1.015\n        plot.annotate(f'{percent}%', (x, y), ha='center', va='bottom')\n\n\n# Lets define the continous and categorical columns\ncat_col  = ['category', 'leather_intr', 'fuel_typ', 'gear_box', 'drive_wheels', 'doors', 'color', 'engine_turbo']\n\n# Plot the bar plot \nr = int(len(cat_col)/2 +1)    # Defining r to autofit the number and size of plots\n\n# Plotting the countplots for each target variable\nplt.figure(figsize=(18,r*3))\nfor n,column in enumerate(cat_col):\n    plot = plt.subplot(r,2,n+1)\n    sns.countplot(df[column]).margins(y=0.15)\n    plot.set_xticklabels(plot.get_xticklabels(), rotation=25, horizontalalignment='right')\n    plt.title(f'{column.title()}',weight='bold')\n    plt.tight_layout()\n    annot_percent(plot)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:28:05.652847Z","iopub.execute_input":"2022-07-12T20:28:05.653347Z","iopub.status.idle":"2022-07-12T20:28:08.159186Z","shell.execute_reply.started":"2022-07-12T20:28:05.653291Z","shell.execute_reply":"2022-07-12T20:28:08.157355Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plot of varierty of cars we make the following conclusions.\n\n1. About 45.44% cars are Sedan, 28.42% are Jeep, 14.79% are Hatchback, and the rest of the categoris has market share of less than 5%.\n2. In 52.55% car varients, the fuel type is petrol, and 21.14% is Diesel and 18.7% is Hybrid, and other fuel type like CNG, LPG, and Hydrogen, and plug-in hybrid are very low.\n3. About 67% of car varients have front drive wheel.\n4. About 72.6% of car varients have leather interior.\n5. About 70% of car varients have automatic gear box.\n6. About 95% of car varients have 4-5 doors.\n7. About 90% of car varients have turbo engine.\n8. Mostly car varients are black (26.13%), white(23.29%) and (19.71%).\n","metadata":{}},{"cell_type":"code","source":"# Create dataframe for model count and percentage\ndf1 = df.copy()  # Select the columns\ndf2 = df1.groupby(by= ['prd_yr'], as_index=False).count().sort_values(by='prd_yr', ascending=True)[['prd_yr','price']]\ndf2 = df2.rename(columns={'price': 'count'})\ndf2['prd_yr_per'] = round(df2['count']/sum(df2['count'])*100,2)\n\n# Plot the bar chart \nplt.figure(figsize=(18,5))\nplot = sns.barplot(x = 'prd_yr', y = 'count',  data = df2)\nplt.title('Year wise variety of cars', fontsize=16)\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90, horizontalalignment='center')\nplt.xlabel('Year of production', fontsize=12)\nplt.ylabel('Number of car varieties', fontsize=12)\n\n# Annonate the bar plot\nfor i,p in enumerate(plot.patches):\n        percentage = df2['prd_yr_per'][i]\n        x = p.get_x() + p.get_width() / 3\n        y = p.get_height()*1.04\n        plot.annotate(f'{percentage}%', (x, y), size = 10, rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:28:19.984810Z","iopub.execute_input":"2022-07-12T20:28:19.985239Z","iopub.status.idle":"2022-07-12T20:28:21.469267Z","shell.execute_reply.started":"2022-07-12T20:28:19.985205Z","shell.execute_reply":"2022-07-12T20:28:21.468058Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plot of varierty of cars we make the following conclusions.\n\n1. The quantity of car varients are gradualy increases year by year and then sharpely decreases after 2014.\n2. The quantity of car varients are maximum from 2012 to 2014 and then it started falling down.","metadata":{}},{"cell_type":"code","source":"# Create dataframe for model count and percentage\nmodel_count_df = df['model'].value_counts().to_frame().reset_index()\nmodel_count_df['model_per'] = round(model_count_df['model']/sum(model_count_df['model'])*100,2)\n\n# Plot the bar chart \nplt.figure(figsize=(18,5))\nplot = sns.barplot(x = 'index', y = 'model',  data = model_count_df.loc[:50,:])\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90, horizontalalignment='center')\nplt.title('Top 50 Models that has highest variety of cars', fontsize=16)\nplt.xlabel('Models', fontsize=12)\nplt.ylabel('Number of varieties', fontsize=12)\n\n# Annonate the bar plot\nfor i,p in enumerate(plot.patches):\n        percentage = model_count_df['model_per'][i]\n        x = p.get_x() + p.get_width() / 2\n        y = p.get_height()*1.02\n        plot.annotate(f'{percentage}%', (x, y), size = 10, rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:28:30.594751Z","iopub.execute_input":"2022-07-12T20:28:30.595217Z","iopub.status.idle":"2022-07-12T20:28:31.738378Z","shell.execute_reply.started":"2022-07-12T20:28:30.595179Z","shell.execute_reply":"2022-07-12T20:28:31.736840Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plot of varierty of cars we make the following conclusions.\n\n1. The models Prius, Sonata, Camry, Elantra has highest varients of cars.\n2. Out of 1590 modles, only these four contributes 21% of car variants","metadata":{}},{"cell_type":"code","source":"# Create dataframe for model count and percentage\nmanufacturer_count_df = df['manufacturer'].value_counts().to_frame().reset_index()\nmanufacturer_count_df['manufacturer_per'] = round(manufacturer_count_df['manufacturer']/sum(manufacturer_count_df['manufacturer'])*100,2)\n\n# Plot the bar chart \nplt.figure(figsize=(18,5))\nplot = sns.barplot(x = 'index', y = 'manufacturer',  data = manufacturer_count_df.loc[:50,:])\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90, horizontalalignment='center')\nplt.title('Top 50 Manufacturers who has highest variety of cars', fontsize=16)\nplt.xlabel('Manufacturer', fontsize=12)\nplt.ylabel('Number of varieties', fontsize=12)\n\n# Annonate the bar plot\nfor i,p in enumerate(plot.patches):\n        percentage = manufacturer_count_df['manufacturer_per'][i]\n        x = p.get_x() + p.get_width() / 2\n        y = p.get_height()*1.02\n        plot.annotate(f'{percentage}%', (x, y), size = 10, rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:28:47.144893Z","iopub.execute_input":"2022-07-12T20:28:47.145339Z","iopub.status.idle":"2022-07-12T20:28:48.328528Z","shell.execute_reply.started":"2022-07-12T20:28:47.145279Z","shell.execute_reply":"2022-07-12T20:28:48.327260Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plot of varierty of cars we make the following conclusions.\n\n1. The manufacturer Hyndai, Toyota, Mercedes-Benz has highest varients of cars.\n2. Out of 65 manufacturer, only these 3 produces 48.8% of car variants.","metadata":{}},{"cell_type":"markdown","source":"# **Bivariate Variable Analysis**","metadata":{}},{"cell_type":"markdown","source":"**Production Year Wise Analysis Of Each Feature**","metadata":{}},{"cell_type":"code","source":"for hue in cat_col:\n    plt.figure(figsize=(18,5))\n    df1 = df[['prd_yr',hue, 'price']].groupby(by= ['prd_yr',hue], as_index=True).count().reset_index()\n    plot = sns.lineplot(x = 'prd_yr', y = 'price',  data = df1, hue = hue)\n    plt.title(f'Year wise variety of cars vs {hue.title()}', fontsize=16,weight='bold')\n    plt.xlabel('Year of production', fontsize=12)\n    plt.ylabel('Variety of cars', fontsize=12)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:28:57.661672Z","iopub.execute_input":"2022-07-12T20:28:57.662112Z","iopub.status.idle":"2022-07-12T20:29:00.804771Z","shell.execute_reply.started":"2022-07-12T20:28:57.662077Z","shell.execute_reply":"2022-07-12T20:29:00.803502Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above line chart we make the following conclusions.\n\n1. The color and category choices for car varients do not changes from start to end.\n2. Mostly car varients has the leather interior but the trend changes from 1990 to 2008.\n3. The choice of petrol fuel type was always the choice.\n4. The automatic gear box started dominating the market after the start of year 2000.\n5. The front drive whell started dominating the market after the start of the year 2000.\n6. The 4-5 doors was always the choice.\n7. Car with no turbo engine always the market dopminator.","metadata":{}},{"cell_type":"code","source":"cat_col","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:29:16.962824Z","iopub.execute_input":"2022-07-12T20:29:16.963448Z","iopub.status.idle":"2022-07-12T20:29:16.970741Z","shell.execute_reply.started":"2022-07-12T20:29:16.963396Z","shell.execute_reply":"2022-07-12T20:29:16.969350Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**Mean price wise analysis of each feature**","metadata":{}},{"cell_type":"code","source":"# Plot the bar plot \nr = int(len(cat_col)/2 +1)    # Defining r to autofit the number and size of plots\n\n# Plotting the countplots for each target variable\nplt.figure(figsize=(20,r*4))\nfor n,column in enumerate(cat_col[:]):\n    df1 = df[['price', column]].groupby(by= [column], as_index=False).agg('mean').sort_values(by='price', ascending=False)\n    plot = plt.subplot(r,2,n+1)\n    sns.barplot(x =df1[column], y= df1['price'])\n    plot.set_xticklabels(plot.get_xticklabels(), rotation=30, horizontalalignment='center')\n    plt.title(f'{column.title()}',weight='bold')\n    plt.tight_layout()\n    annot_percent(plot)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:29:25.674001Z","iopub.execute_input":"2022-07-12T20:29:25.674529Z","iopub.status.idle":"2022-07-12T20:29:28.385403Z","shell.execute_reply.started":"2022-07-12T20:29:25.674469Z","shell.execute_reply":"2022-07-12T20:29:28.384219Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plots of mean price of cars we make the following conclusions.\n\n1. The average price of plug-in Hybrid is maximum and for the CNG is the lowest. The price follow the trend: plug-in Hybrid > Diesel > Hydrogen > Petrol > LPG > Hybrid > CNG\n2. The average price of tiptronic gear box is maximum and for the manual the average price is the lowest.\n3. The average price for 4x4, front, and rear drive wheels are the almost same.\n4. The average price of 5+ doors is maximum and for the 2-3 doors the average price is the lowest.\n5. The average price of yellow color is maximum and for the purple color the average price is the lowest.\n6. The average price of cars with turbo engine is higher.","metadata":{}},{"cell_type":"code","source":"# Create the dataframe\ndf1 = df[['price', 'manufacturer']].groupby(by= ['manufacturer'], as_index=False).agg('mean').sort_values(by='price', ascending=False)\n\n# Lets barplot for each categorical columns\nplt.figure(figsize=(50,10))\n\nplot = sns.barplot(x=\"manufacturer\", y=\"price\", data=df1)\nplot.set_xticklabels(plot.get_xticklabels(), rotation=30, horizontalalignment='center')\nplt.title('Bar plot of average price for each category of manufacturer', fontsize=40)\nplt.xlabel('Manufacturer', fontsize=30)\nplt.ylabel('Mean Price', fontsize=30)\n\n# #Drawing a horizontal for each quantile\n# for i in list(df1[\"price\"].quantile([0,0.25,0.5,0.75, 1.00])):\n#     plot.axhline(i)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:29:39.164455Z","iopub.execute_input":"2022-07-12T20:29:39.164870Z","iopub.status.idle":"2022-07-12T20:29:40.392055Z","shell.execute_reply.started":"2022-07-12T20:29:39.164836Z","shell.execute_reply":"2022-07-12T20:29:40.390784Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plots of mean price of cars we make the following conclusions. The Average price for the following manufacturer is highest.\n\n1. Aston Martin\n2. Testla\n3. Bentley\n4. Lamborghin","metadata":{}},{"cell_type":"code","source":"# Create the dataframe\ndf1 = df[['price', 'model']].groupby(by= ['model'], as_index=False).agg('mean').sort_values(by='price', ascending=False)\n\n# Lets barplot for each categorical columns\nplt.figure(figsize=(50,10))\n\nplot = sns.barplot(x=\"model\", y=\"price\", data=df1[:])\nplot.set_xticklabels(plot.get_xticklabels(), rotation=30, horizontalalignment='center')\nplt.title('Bar plot of average price for each category of model', fontsize=40)\nplt.xlabel('model', fontsize=30)\nplt.ylabel('Mean Price', fontsize=30)\n\n# #Drawing a horizontal for each quantile\n# for i in list(df1[\"price\"].quantile([0,0.25,0.5,0.75, 1.00])):\n#     plot.axhline(i)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:29:49.611179Z","iopub.execute_input":"2022-07-12T20:29:49.611730Z","iopub.status.idle":"2022-07-12T20:30:12.842741Z","shell.execute_reply.started":"2022-07-12T20:29:49.611693Z","shell.execute_reply":"2022-07-12T20:30:12.841270Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Create the dataframe\ndf1 = df[['price', 'prd_yr']].groupby(by= ['prd_yr'], as_index=False).agg('mean').sort_values(by='price', ascending=False)\n\n# Lets barplot for each categorical columns\nplt.figure(figsize=(18,5))\nplot = sns.barplot(x=\"prd_yr\", y=\"price\", data=df1)\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90, horizontalalignment='center')\nplt.title('Bar plot of average price for each production year', fontsize=16)\nplt.xlabel('Production year', fontsize=12)\nplt.ylabel('Mean Price', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:06.304933Z","iopub.execute_input":"2022-07-12T20:31:06.305364Z","iopub.status.idle":"2022-07-12T20:31:07.184626Z","shell.execute_reply.started":"2022-07-12T20:31:06.305328Z","shell.execute_reply":"2022-07-12T20:31:07.183372Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# The cars from 1973, 1977, 1982, 1987\ndf1 = df[(df['prd_yr'] == 1973) | (df['prd_yr'] == 1977) | (df['prd_yr'] == 1982) | (df['prd_yr'] == 1987)].sort_values(by='price', ascending=False)\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:10.712696Z","iopub.execute_input":"2022-07-12T20:31:10.713179Z","iopub.status.idle":"2022-07-12T20:31:10.750514Z","shell.execute_reply.started":"2022-07-12T20:31:10.713138Z","shell.execute_reply":"2022-07-12T20:31:10.749624Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above bar plots of mean price of cars we make the following conclusions.\n\n1. Generally, the average price of cars is increasing every year.\n2. In the year, 1973, 1977, 1982, the average price of the car is very highly unusual. It will act as outlier in our ML model, so its better to delete these values.","metadata":{}},{"cell_type":"code","source":"# The price of cars that belogs to year 1973, 1977, 1982, 1987 and have price above 5000. \ndf2 = df1[df1['price']>10000]\n# Removing these values from our main dataset\ndf = df.drop(df2.index)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:15.122875Z","iopub.execute_input":"2022-07-12T20:31:15.123349Z","iopub.status.idle":"2022-07-12T20:31:15.135127Z","shell.execute_reply.started":"2022-07-12T20:31:15.123311Z","shell.execute_reply":"2022-07-12T20:31:15.133718Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# The jointplot of distribution, scatter plot, and the regression line of mean price and variour numercal columns is given below.\nfor col in cont_col[1:]:\n    g = sns.JointGrid()\n    sns.regplot(x=df[col], y=df[\"price\"],line_kws={\"color\": \"red\"}, ax=g.ax_joint)\n    sns.kdeplot(y=df[\"price\"], linewidth=2, ax=g.ax_marg_y)\n    sns.kdeplot(x=df[col], linewidth=2, ax=g.ax_marg_x)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:18.332641Z","iopub.execute_input":"2022-07-12T20:31:18.333074Z","iopub.status.idle":"2022-07-12T20:31:26.291834Z","shell.execute_reply.started":"2022-07-12T20:31:18.333040Z","shell.execute_reply":"2022-07-12T20:31:26.290389Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# **Multi Variable Analysis**","metadata":{}},{"cell_type":"code","source":"# Correlation Analysis\nplt.figure(figsize=(18,9))\nplot = sns.heatmap(abs(df.corr()), annot=True, cmap='coolwarm',vmin=-1)\nplt.title('Correlation Heatmap of Car price prediction Dataset', weight='bold')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=0, horizontalalignment='center')\nplot.set_yticklabels(plot.get_yticklabels(), rotation=0, horizontalalignment='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:31.984612Z","iopub.execute_input":"2022-07-12T20:31:31.986116Z","iopub.status.idle":"2022-07-12T20:31:32.610549Z","shell.execute_reply.started":"2022-07-12T20:31:31.986062Z","shell.execute_reply":"2022-07-12T20:31:32.609277Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Correlation Analysis only for target variabel \nTarget_corr  = df.corr().loc[:, 'price'].to_frame().sort_values(by = 'price' , ascending=False).T\nplt.figure(figsize=(20,1))\nplot = sns.heatmap(Target_corr, annot=True, cmap='coolwarm', vmin = -1)\nplt.title('Correlation Heatmap of Car price prediction Dataset', weight='bold')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=0, horizontalalignment='center')\nplot.set_yticklabels(plot.get_yticklabels(), rotation=0, horizontalalignment='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:36.164016Z","iopub.execute_input":"2022-07-12T20:31:36.164515Z","iopub.status.idle":"2022-07-12T20:31:36.469592Z","shell.execute_reply.started":"2022-07-12T20:31:36.164477Z","shell.execute_reply":"2022-07-12T20:31:36.468323Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above correlation heatmap, we make the following conclusions.\n\n1. The price has high positive correlation with production year, and engine turbo.\n2. The price has high negative correlation with mileage.\n3. The price has very little correlation with engine volume, cylinder, levy, and airbags.\n4. The produduction year and levy have high positive correlation.\n5. The engive vol and cylinder have high positive correlation.","metadata":{}},{"cell_type":"code","source":"# Plot the line chart \nfor Hue in cat_col:\n    plt.figure(figsize=(18,5))\n    plot = sns.lineplot(x = 'prd_yr', y = 'price',  data = df, hue = Hue,ci=None)\n    plt.title(f'Year wise variety mean price of cars vs {Hue.title()}', fontsize=16,weight='bold')\n    plt.xlabel('Year of production', fontsize=12)\n    plt.ylabel('Mean price', fontsize=12)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:40.722936Z","iopub.execute_input":"2022-07-12T20:31:40.723389Z","iopub.status.idle":"2022-07-12T20:31:43.810600Z","shell.execute_reply.started":"2022-07-12T20:31:40.723350Z","shell.execute_reply":"2022-07-12T20:31:43.809295Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions:** From the above line plots of mean price of cars year wise we make the following conclusions.\n\n1. The average price of leather interior is always higher.\n2. Intially there were petrol fuel cars therefore the price of the petrol fuel cars were very high. But after aprox 1995 the average price of petrol cars become cheaper and diesel cars become costlier.\n3. Initally the average price of automatic gear box was so high but it become cheaper as the time passes.\n4. The 5+ door cars came after aprox 1995 and there average price were always higher.\n5. The average price of turbo engine is always higher.","metadata":{}},{"cell_type":"markdown","source":"# **Feature Extraction And Preparing The Dataset**","metadata":{}},{"cell_type":"markdown","source":"**Removing Multicolinearity**","metadata":{}},{"cell_type":"code","source":"# Check for the Multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"Variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif = vif.set_index('Variables')\n\n    return(vif)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:31:57.544767Z","iopub.execute_input":"2022-07-12T20:31:57.545224Z","iopub.status.idle":"2022-07-12T20:31:57.554064Z","shell.execute_reply.started":"2022-07-12T20:31:57.545186Z","shell.execute_reply":"2022-07-12T20:31:57.552399Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"df_prepar = df.copy().drop(['price'], axis = 1)\nprint(\"VIF without removing anything:\")\nprint(calc_vif(df_prepar[[i for i in df_prepar.describe().columns]]))\nprint(\"=\"*40+\"||\")\ndf_prepar = df_prepar.drop(['cylinders'], axis = 1)\nprint(\"VIF after removing 'cylinders'\")\nprint(calc_vif(df_prepar[[i for i in df_prepar.describe().columns]]))","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:32:07.443926Z","iopub.execute_input":"2022-07-12T20:32:07.444506Z","iopub.status.idle":"2022-07-12T20:32:08.013224Z","shell.execute_reply.started":"2022-07-12T20:32:07.444467Z","shell.execute_reply":"2022-07-12T20:32:08.011781Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"**Feature engineering:**\n\n\nMethod 1: Using one hot incoding of top frequenct variables of Manufacture, Model.\n\nMethod 2: Using target variables to create bins for model and manufacture.\n\nMethod 3: Frequency label encoding of model and manufacture.\n","metadata":{}},{"cell_type":"code","source":"# Create the new dataframe and drop collinear column\ndf_prepar = df.copy().drop(['price', 'cylinders'], axis = 1)\n\n# Do label encoding for leather interior\ndf_prepar = df_prepar.replace({'leather_intr':{'Yes':1, 'No':0}})\n\n# One hot encoding for the catagorical columns \nX = pd.get_dummies(df_prepar, columns=['category','fuel_typ','gear_box','drive_wheels','doors','wheel','color'], prefix=['category','fuel_typ','gear_box','drive_wheels','doors','wheel','color'])\ny = df['price']","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:32:21.733476Z","iopub.execute_input":"2022-07-12T20:32:21.734725Z","iopub.status.idle":"2022-07-12T20:32:21.791112Z","shell.execute_reply.started":"2022-07-12T20:32:21.734677Z","shell.execute_reply":"2022-07-12T20:32:21.789848Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"**Encodign: Using method 1**\n\nUsing one hot incoding of top frequenct variables of Manufacture, Model.","metadata":{}},{"cell_type":"code","source":"'''\nMethod 1: Using one hot incoding of top frequent variables of Manufacture, Model\n'''\n\nX_1 = X.copy()\n# let's make a list with the most frequent categories of the variable\n\ntop_20_model = [y for y in df.model.value_counts().sort_values(ascending=False).head(20).index]\ntop_20_manufacturer = [y for y in df.manufacturer.value_counts().sort_values(ascending=False).head(20).index]\n# top_20_prd_yr = [y for y in df.prd_yr.value_counts().sort_values(ascending=False).head(20).index]\n\n\n# get whole set of dummy variables, for all the categorical variables\ndef one_hot_encoding_top_x(df, variable, top_x_labels):\n    # function to create the dummy variables for the most frequent labels\n    # we can vary the number of most frequent labels that we encode\n    \n    for label in top_x_labels:\n        df[variable+'_'+str(label)] = np.where(df[variable]==label, 1, 0)\n\n# encode X2 into the 10 most frequent categories\none_hot_encoding_top_x(X_1, 'model', top_20_model)\none_hot_encoding_top_x(X_1, 'manufacturer', top_20_manufacturer)\n# one_hot_encoding_top_x(X_1, 'prd_yr', top_20_prd_yr)\n\n# Drop manufacturer, model, and prd_yr\nX_1 = X_1.drop(['manufacturer', 'model'], axis=1)\n# X_1 = X_1.drop(['prd_yr'], axis=1)\n\nX_1.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:32:42.936449Z","iopub.execute_input":"2022-07-12T20:32:42.937618Z","iopub.status.idle":"2022-07-12T20:32:43.134414Z","shell.execute_reply.started":"2022-07-12T20:32:42.937561Z","shell.execute_reply":"2022-07-12T20:32:43.133254Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Encoding: Using method 2**\n\n\nUsing target variables to create bins for model and manufacture.","metadata":{}},{"cell_type":"code","source":"'''\nMethod 2: Using target variables to create bins for model and manufacture.\n'''\nX_2 = X.copy()\n\n# let's make a list with the most frequent categories of the variable\ntop_20_prd_yr = [y for y in df.prd_yr.value_counts().sort_values(ascending=False).head(20).index]\n\n\n# get whole set of dummy variables, for all the categorical variables\ndef one_hot_encoding_top_x(df, variable, top_x_labels):\n    # function to create the dummy variables for the most frequent labels\n    # we can vary the number of most frequent labels that we encode\n    \n    for label in top_x_labels:\n        df[variable+'_'+str(label)] = np.where(df[variable]==label, 1, 0)\n\n# encode X2 into the 20 most frequent categories\n# one_hot_encoding_top_x(X_2, 'prd_yr', top_20_prd_yr)\n\n# Target encoding for model and manufacture\ndef target_encoding(val):\n    if 0 <= val and  val < 10000:\n        return  1\n    if 10000 <= val and  val < 20000:\n        return  2\n    if 20000 <= val and  val < 30000:\n        return  3\n    if 30000 <= val:\n        return  4\n\n    \n\n    \n    \n\n# Target Encode for manufactuer\ndf1 = df[['price', 'manufacturer']].groupby(by= ['manufacturer'], as_index=False).agg('mean').sort_values(by='price', ascending=False)\ndf1['manufacturer_encode'] = df1['price'].apply(lambda x : target_encoding(x))\n\nmapping = dict(df1[['manufacturer', 'manufacturer_encode']].values)\nX_2['manufacturer_encode'] = df_prepar.manufacturer.map(mapping)\n\n\n# Target Encode for model\ndf1 = df[['price', 'model']].groupby(by= ['model'], as_index=False).agg('mean').sort_values(by='price', ascending=False)\ndf1['model_encode'] = df1['price'].apply(lambda x : target_encoding(x))\n\nmapping = dict(df1[['model', 'model_encode']].values)\nX_2['model_encode'] = df_prepar.model.map(mapping)\n\n\n# Drop manufacturer, model, and prd_yr\nX_2 = X_2.drop(['manufacturer', 'model'], axis=1)\n# X_2 = X_2.drop(['prd_yr'], axis=1)\nX_2.head()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:33:31.508819Z","iopub.execute_input":"2022-07-12T20:33:31.510106Z","iopub.status.idle":"2022-07-12T20:33:31.584694Z","shell.execute_reply.started":"2022-07-12T20:33:31.510053Z","shell.execute_reply":"2022-07-12T20:33:31.583396Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"**Encoding: Using method 3**\n\nFrequency label encoding of model and manufacture.","metadata":{}},{"cell_type":"code","source":"'''\nMethod 3: Frequency label encoding of model and manufacture.\n'''\nX_3 = X.copy()\n\n# # let's make a list with the most frequent categories of the variable\n# top_20_prd_yr = [y for y in df.prd_yr.value_counts().sort_values(ascending=False).head(20).index]\n\n# # encode X3 into the 20 most frequent categories\n# one_hot_encoding_top_x(X_3, 'prd_yr', top_20_prd_yr)\n\n# Frequncy encoding for model and manufacture\nlabelencoder = LabelEncoder()\nX_3['manufacturer']=labelencoder.fit_transform(X_3['manufacturer'])\nX_3['model']=labelencoder.fit_transform(X_3['model'])\n# X_3 = X_3.drop(['prd_yr'], axis=1)\n\nX_3.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:33:44.029610Z","iopub.execute_input":"2022-07-12T20:33:44.030026Z","iopub.status.idle":"2022-07-12T20:33:44.077401Z","shell.execute_reply.started":"2022-07-12T20:33:44.029991Z","shell.execute_reply":"2022-07-12T20:33:44.076081Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning**","metadata":{}},{"cell_type":"markdown","source":"**Defining Some Functions**","metadata":{}},{"cell_type":"code","source":"# Defining a function to train the input model and print evaluation matrix\ndef analyse_model(model, X_train, X_test, y_train, y_test, plotgraph = True):\n\n    '''Takes regressor model and train test splits as input and prints the\n    evaluation matrices with the plot and returns the model'''\n\n    # Fitting the model\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    a,p = y_test,y_pred\n\n    # Calculating Evaluation Matrix\n    mse = mean_squared_error(a,p)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(a,p)\n    try:\n        try:\n            importance = model.feature_importances_\n            feature = independent_variables\n        except:\n            importance = np.abs(model.coef_)\n            feature = independent_variables\n        indices = np.argsort(importance)\n        indices = indices[::-1]\n    except:\n        pass\n    \n    # Printing Evaluation Matrix\n    print(\"MSE         :\" , round(mse,2))\n    print(\"RMSE        :\" , round(rmse,2))\n    print(\"MAE         :\" , round(mean_absolute_error(a,p),2))\n    print(\"Train R2    :\" , round(r2_score(y_train,model.predict(X_train)),2) ) \n    print(\"Test R2     :\" , round(r2,2))\n    print(\"Adjusted R2 :\", round(1-(1-r2)*((len(X_test)-1)/(len(X_test)-X_test.shape[1]-1)),2) )\n    \n    if plotgraph:\n        # Plotting actual and predicted values and the feature importances:\n        plt.figure(figsize=(18,6))\n\n        plt.plot((y_pred)[:100])\n        plt.plot((np.array(y_test)[:100]))\n        plt.legend([\"Predicted\",\"Actual\"])\n        plt.title('Actual and Predicted Car Price')\n        plt.show()\n        try:\n            plt.figure(figsize=(18,6))\n            plt.bar(range(len(indices)),importance[indices])\n            plt.xticks(range(len(indices)), [feature[i] for i in indices],  rotation=90,horizontalalignment='center')\n\n            plt.title('Feature Importance')\n            plt.tight_layout()\n            plt.show()\n        except:\n            pass\n\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:34:56.522788Z","iopub.execute_input":"2022-07-12T20:34:56.523257Z","iopub.status.idle":"2022-07-12T20:34:56.542434Z","shell.execute_reply.started":"2022-07-12T20:34:56.523221Z","shell.execute_reply":"2022-07-12T20:34:56.541171Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"**Apply Machine learning algorithms using encoding method 1**","metadata":{}},{"cell_type":"markdown","source":"# **Split the data: Encoding Method 1**","metadata":{}},{"cell_type":"code","source":"# Let get the independent_variables data\nX = X_1.copy()\n\n# Define the independent_variables\nindependent_variables = [i for i in X.columns]\n\n# Split the data in train and  test\nX_train, X_test, y_train, y_test = train_test_split( X,y , test_size = 0.2, random_state = 20)\n\n# Scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:35:09.900093Z","iopub.execute_input":"2022-07-12T20:35:09.900546Z","iopub.status.idle":"2022-07-12T20:35:09.966470Z","shell.execute_reply.started":"2022-07-12T20:35:09.900507Z","shell.execute_reply":"2022-07-12T20:35:09.965383Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# **Regression Algorithm: Encoding Method 1:**","metadata":{}},{"cell_type":"code","source":"# Fitting Linear Regression Model\nlr1 = LinearRegression()\nanalyse_model(lr1, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:35:18.607943Z","iopub.execute_input":"2022-07-12T20:35:18.608389Z","iopub.status.idle":"2022-07-12T20:35:20.784602Z","shell.execute_reply.started":"2022-07-12T20:35:18.608340Z","shell.execute_reply":"2022-07-12T20:35:20.783395Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# **Random Forest: Encoding Method 1:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = RandomForestRegressor(random_state=2)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [200,500,1000]}\n\n# GridSearch to find the best parameters\nrf1 = GridSearchCV(regressor, \n                  param_grid = grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv=5,\n                  n_jobs = -1)\nrf1.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(rf1.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:40:51.808747Z","iopub.execute_input":"2022-07-12T20:40:51.810268Z","iopub.status.idle":"2022-07-12T20:50:23.875185Z","shell.execute_reply.started":"2022-07-12T20:40:51.810178Z","shell.execute_reply":"2022-07-12T20:50:23.873624Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# **Gradient Boosting: Encoding Method 1:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = GradientBoostingRegressor(random_state=4)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [150,500],\n        'max_depth' : [10,15]}\n\n# GridSearch to find the best parameters\ngbr1 = GridSearchCV(regressor, \n                   param_grid = grid, \n                   scoring = 'neg_mean_squared_error', \n                   cv=5,\n                   n_jobs = -1)\n\ngbr1.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(gbr1.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T20:50:47.279034Z","iopub.execute_input":"2022-07-12T20:50:47.279517Z","iopub.status.idle":"2022-07-12T20:56:44.087836Z","shell.execute_reply.started":"2022-07-12T20:50:47.279475Z","shell.execute_reply":"2022-07-12T20:56:44.086911Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# **XGBoost: Encoding Method 1:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = XGBRegressor()\n\n# HYperparameter Grid\ngrid = {\n    'max_depth': [10,15],\n    'min_child_weight': [1,5],\n    'colsample_bytree': [0.7,1],\n    'n_estimators' : [150,500],\n    'objective': ['reg:squarederror']\n}\n\n\n# GridSearch to find the best parameters\nxgb1 = GridSearchCV(estimator = regressor,\n                       param_grid = grid,                        \n                       scoring = 'neg_mean_squared_error',\n                       cv = 5,\n                       n_jobs = -1,\n                       verbose = 1)\n\n# Fit the train data in the model\nxgb1.fit(X_train,y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(xgb1.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:13:26.309515Z","iopub.execute_input":"2022-07-12T21:13:26.310041Z","iopub.status.idle":"2022-07-12T21:31:00.802678Z","shell.execute_reply.started":"2022-07-12T21:13:26.310001Z","shell.execute_reply":"2022-07-12T21:31:00.801205Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"# **KNN Regressor: Encoding Method 1:**","metadata":{}},{"cell_type":"code","source":"# In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\n\nknn = KNeighborsRegressor()\n# defining parameter range\n\nknn_cv1= GridSearchCV(knn,param_grid,\n                     cv=5,\n                     n_jobs = -1,\n                     scoring = 'neg_mean_squared_error',\n                     verbose=1)\n# fitting the model for grid search\nknn_cv1.fit(X_train,y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(knn_cv1.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:31:13.164510Z","iopub.execute_input":"2022-07-12T21:31:13.164973Z","iopub.status.idle":"2022-07-12T21:32:50.509708Z","shell.execute_reply.started":"2022-07-12T21:31:13.164932Z","shell.execute_reply":"2022-07-12T21:32:50.508715Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# **Apply Machine learning algorithms using encoding method 2**","metadata":{}},{"cell_type":"markdown","source":"**Split the data: Encoding Method 2:**","metadata":{}},{"cell_type":"code","source":"# Let get the independent_variables data\nX = X_2.copy()\n\n# Define the independent_variables\nindependent_variables = [i for i in X.columns]\n\n# Split the data in train and  test\nX_train, X_test, y_train, y_test = train_test_split( X,y , test_size = 0.2, random_state = 20)\n\n# Scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:33:01.841496Z","iopub.execute_input":"2022-07-12T21:33:01.842354Z","iopub.status.idle":"2022-07-12T21:33:01.914176Z","shell.execute_reply.started":"2022-07-12T21:33:01.842284Z","shell.execute_reply":"2022-07-12T21:33:01.912870Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"**Regression algorithm: Encoding Method 2:**","metadata":{}},{"cell_type":"code","source":"# Fitting Linear Regression Model\nlr2 = LinearRegression()\nanalyse_model(lr2, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:33:11.772188Z","iopub.execute_input":"2022-07-12T21:33:11.772656Z","iopub.status.idle":"2022-07-12T21:33:12.742745Z","shell.execute_reply.started":"2022-07-12T21:33:11.772611Z","shell.execute_reply":"2022-07-12T21:33:12.741389Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest: Encoding Method 2:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = RandomForestRegressor(random_state=2)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [200,500,1000]}\n\n# GridSearch to find the best parameters\nrf2 = GridSearchCV(regressor, \n                  param_grid = grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv=5,\n                  n_jobs = -1)\nrf2.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(rf2.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:33:20.131432Z","iopub.execute_input":"2022-07-12T21:33:20.131844Z","iopub.status.idle":"2022-07-12T21:39:08.878689Z","shell.execute_reply.started":"2022-07-12T21:33:20.131812Z","shell.execute_reply":"2022-07-12T21:39:08.877549Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"**Gradient Boosting: Encoding Method 2**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = GradientBoostingRegressor(random_state=4)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [150,500],\n        'max_depth' : [10,15]}\n\n# GridSearch to find the best parameters\ngbr2 = GridSearchCV(regressor, \n                   param_grid = grid, \n                   scoring = 'neg_mean_squared_error', \n                   cv=5,\n                   n_jobs = -1)\n\ngbr2.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(gbr2.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:39:15.010951Z","iopub.execute_input":"2022-07-12T21:39:15.012694Z","iopub.status.idle":"2022-07-12T21:43:22.805653Z","shell.execute_reply.started":"2022-07-12T21:39:15.012639Z","shell.execute_reply":"2022-07-12T21:43:22.804010Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost: Encoding Method 2:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = XGBRegressor()\n\n# HYperparameter Grid\ngrid = {\n    'max_depth': [10,15],\n    'min_child_weight': [1,5],\n    'colsample_bytree': [0.7,1],\n    'n_estimators' : [150,500],\n    'objective': ['reg:squarederror']\n}\n\n\n# GridSearch to find the best parameters\nxgb2 = GridSearchCV(estimator = regressor,\n                       param_grid = grid,                        \n                       scoring = 'neg_mean_squared_error',\n                       cv = 5,\n                       n_jobs = -1,\n                       verbose = 1)\n\n# Fit the train data in the model\nxgb2.fit(X_train,y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(xgb2.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:43:45.496169Z","iopub.execute_input":"2022-07-12T21:43:45.496897Z","iopub.status.idle":"2022-07-12T21:56:02.001133Z","shell.execute_reply.started":"2022-07-12T21:43:45.496837Z","shell.execute_reply":"2022-07-12T21:56:02.000059Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"**KNN Regressor: Encoding Method 2:**","metadata":{}},{"cell_type":"code","source":"# In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\n\nknn = KNeighborsRegressor()\n# defining parameter range\n\nknn_cv2= GridSearchCV(knn,param_grid,\n                     cv=5,\n                     n_jobs = -1,\n                     scoring = 'neg_mean_squared_error',\n                     verbose=1)\n# fitting the model for grid search\nknn_cv2.fit(X_train,y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(knn_cv2.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:57:42.203878Z","iopub.execute_input":"2022-07-12T21:57:42.204290Z","iopub.status.idle":"2022-07-12T21:59:15.687999Z","shell.execute_reply.started":"2022-07-12T21:57:42.204258Z","shell.execute_reply":"2022-07-12T21:59:15.686539Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"# **Apply Machine learning algorithms using encoding method 3**","metadata":{}},{"cell_type":"markdown","source":"**Split the data: Encoding Method 3:**","metadata":{}},{"cell_type":"code","source":"# Let get the independent_variables data\nX = X_3.copy()\n\n# Define the independent_variables\nindependent_variables = [i for i in X.columns]\n\n# Split the data in train and  test\nX_train, X_test, y_train, y_test = train_test_split( X,y , test_size = 0.2, random_state = 20)\n\n# Scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:59:28.042862Z","iopub.execute_input":"2022-07-12T21:59:28.044022Z","iopub.status.idle":"2022-07-12T21:59:28.095548Z","shell.execute_reply.started":"2022-07-12T21:59:28.043965Z","shell.execute_reply":"2022-07-12T21:59:28.094154Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"**Regression algorithm: Encoding Method 3**","metadata":{}},{"cell_type":"code","source":"# Fitting Linear Regression Model\nlr3 = LinearRegression()\nanalyse_model(lr3, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:59:38.925576Z","iopub.execute_input":"2022-07-12T21:59:38.927358Z","iopub.status.idle":"2022-07-12T21:59:40.320765Z","shell.execute_reply.started":"2022-07-12T21:59:38.927281Z","shell.execute_reply":"2022-07-12T21:59:40.319390Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest: Encoding Method 3:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = RandomForestRegressor(random_state=2)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [200,500,1000]}\n\n# GridSearch to find the best parameters\nrf3 = GridSearchCV(regressor, \n                  param_grid = grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv=5,\n                  n_jobs = -1)\nrf3.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(rf3.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T22:00:07.567178Z","iopub.execute_input":"2022-07-12T22:00:07.567976Z","iopub.status.idle":"2022-07-12T22:08:11.442015Z","shell.execute_reply.started":"2022-07-12T22:00:07.567933Z","shell.execute_reply":"2022-07-12T22:08:11.440542Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Regressor\nregressor = RandomForestRegressor(random_state=2)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [200,500,1000]}\n\n# GridSearch to find the best parameters\nrf3 = GridSearchCV(regressor, \n                  param_grid = grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv=5,\n                  n_jobs = -1)\nrf3.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(rf3.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T22:08:39.232211Z","iopub.execute_input":"2022-07-12T22:08:39.232671Z","iopub.status.idle":"2022-07-12T22:16:30.006826Z","shell.execute_reply.started":"2022-07-12T22:08:39.232629Z","shell.execute_reply":"2022-07-12T22:16:30.005292Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"**Gradient Boosting: Encoding Method 3:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = GradientBoostingRegressor(random_state=4)\n\n# HYperparameter Grid\ngrid = {'n_estimators' : [150,500],\n        'max_depth' : [10,15]}\n\n# GridSearch to find the best parameters\ngbr3 = GridSearchCV(regressor, \n                   param_grid = grid, \n                   scoring = 'neg_mean_squared_error', \n                   cv=5,\n                   n_jobs = -1)\n\ngbr3.fit(X_train, y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(gbr3.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T22:16:35.904964Z","iopub.execute_input":"2022-07-12T22:16:35.905467Z","iopub.status.idle":"2022-07-12T22:21:15.900917Z","shell.execute_reply.started":"2022-07-12T22:16:35.905424Z","shell.execute_reply":"2022-07-12T22:21:15.899360Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost: Encoding Method 3:**","metadata":{}},{"cell_type":"code","source":"# Regressor\nregressor = XGBRegressor()\n\n# HYperparameter Grid\ngrid = {\n    'max_depth': [10,15],\n    'min_child_weight': [1,5],\n    'colsample_bytree': [0.7,1],\n    'n_estimators' : [150,500],\n}\n\n\n# GridSearch to find the best parameters\nxgb3 = GridSearchCV(estimator = regressor,\n                       param_grid = grid,                        \n                       scoring = 'neg_mean_squared_error',\n                       cv = 5,\n                       n_jobs = -1,\n                       verbose = 1)\n\n# Fit the train data in the model\nxgb3.fit(X_train,y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(xgb3.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T22:21:23.373006Z","iopub.execute_input":"2022-07-12T22:21:23.373567Z","iopub.status.idle":"2022-07-12T22:33:55.159846Z","shell.execute_reply.started":"2022-07-12T22:21:23.373523Z","shell.execute_reply":"2022-07-12T22:33:55.157711Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"**KNN Regressor: Encoding Method 3:**","metadata":{}},{"cell_type":"code","source":"# In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\n\nknn = KNeighborsRegressor()\n# defining parameter range\n\nknn_cv3= GridSearchCV(knn,param_grid,\n                     cv=5,\n                     n_jobs = -1,\n                     scoring = 'neg_mean_squared_error',\n                     verbose=1)\n# fitting the model for grid search\nknn_cv3.fit(X_train,y_train)\n\n# Analysing the model with best set of parametes\nanalyse_model(knn_cv3.best_estimator_, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-12T22:37:56.307958Z","iopub.execute_input":"2022-07-12T22:37:56.309939Z","iopub.status.idle":"2022-07-12T22:39:28.198138Z","shell.execute_reply.started":"2022-07-12T22:37:56.309857Z","shell.execute_reply":"2022-07-12T22:39:28.196850Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input","metadata":{"execution":{"iopub.status.busy":"2022-07-12T23:11:30.926447Z","iopub.execute_input":"2022-07-12T23:11:30.927316Z","iopub.status.idle":"2022-07-12T23:11:31.823599Z","shell.execute_reply.started":"2022-07-12T23:11:30.927256Z","shell.execute_reply":"2022-07-12T23:11:31.821933Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"Image(\"../input/images-matrics/metrics performance .png\")","metadata":{"execution":{"iopub.status.busy":"2022-07-12T23:53:43.285822Z","iopub.execute_input":"2022-07-12T23:53:43.286834Z","iopub.status.idle":"2022-07-12T23:53:43.309603Z","shell.execute_reply.started":"2022-07-12T23:53:43.286777Z","shell.execute_reply":"2022-07-12T23:53:43.308330Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"**Important Points**\n\n1. Out of all the models, the Random forest model with encoding method 3 gives the best result and the second runner up model is Gradient boosting with encoding method 3.\n2. For the both the models, the R2 for the test data is 0.80.\n3. The Random forest model with encoding method 3 has the lowest mean square error, root mean square error, mean absolute error, and highest R2 and adjusted R2 square value.\n4. It is quite impressive that both the random forest model and gradient bossting are two different types of ensembling technique but both the results gives almost the similar and top results.\n5. For the Random forest with encoding method 3 the 10 most important features are in the order: Production year > airbags > mileage > levy > engine volume > fuel type diesel > model > gearbox tiptronic > drive wheels front > gearbox automatic\n6. For the Gradient boosting with encoding method 3 the 10 most important features are in the order: Production year > airbags > mileage > levy > fuel type diesel > engine volume > model > gearbox tiptronic > drive wheels front > gearbox automatic\n7. For both the top two regression models, the top 10 decisive features are almost in the same order.\n8. Using grid search CV for random forest with encoding method 3 the best param are 'max_depth': 10, 'n_estimators': 150.\n9. Using grid search CV for gradient boosting with encoding method 3 the best param are 'max_depth': 10, 'n_estimators': 150","metadata":{}}]}